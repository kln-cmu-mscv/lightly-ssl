{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srinitca/anaconda3/envs/lightly/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import lightly\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "num_workers = 8\n",
    "batch_size = 1\n",
    "seed = 1\n",
    "max_epochs = 20\n",
    "input_size = 128\n",
    "num_ftrs = 32\n",
    "\n",
    "pl.seed_everything(seed)\n",
    "\n",
    "path_to_data = '/home/srinitca/vlr/project/ARID_Dataset/clips_v1.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = lightly.data.SimCLRCollateFunction(\n",
    "    input_size=input_size,\n",
    "    vf_prob=0.5,\n",
    "    rr_prob=0.5\n",
    ")\n",
    "\n",
    "# We create a torchvision transformation for embedding the dataset after \n",
    "# training\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=lightly.data.collate.imagenet_normalize['mean'],\n",
    "        std=lightly.data.collate.imagenet_normalize['std'],\n",
    "    )\n",
    "])\n",
    "\n",
    "dataset_train_simclr = lightly.data.LightlyDataset(\n",
    "    input_dir=path_to_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_simclr = torch.utils.data.DataLoader(\n",
    "    dataset_train_simclr,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train_simclr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([496])\n",
      "['Walk/Walk_16_23-096-mp4.png']\n"
     ]
    }
   ],
   "source": [
    "print(batch[1])\n",
    "print(batch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srinitca/anaconda3/envs/lightly/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:VideoIter:: >> `check_video' is off, `tolerant_corrupted_video' is automatically activated.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from data import iterator_factory as iter_fac\n",
    "import torch\n",
    "\n",
    "dataset = \"ARID\"\n",
    "batch_size = 4\n",
    "clip_length = 16\n",
    "train_frame_interval = 2\n",
    "data_root = \"./dataset/ARID\"\n",
    "config = {}\n",
    "config['mean'] = [0.43216, 0.394666, 0.37645] \n",
    "config['std'] = [0.22803, 0.22145, 0.216989] \n",
    "input_conf = config\n",
    "resume_epoch=0\n",
    "return_item_subpath=True\n",
    "iter_seed = 0 #torch.initial_seed() + 100 + max(0, resume_epoch) * 100\n",
    "\n",
    "train_iter = iter_fac.creat(name=dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            clip_length=clip_length, \n",
    "                            train_interval=train_frame_interval,\n",
    "                            mean=input_conf['mean'], std=input_conf['std'], \n",
    "                            seed=iter_seed, \n",
    "                            data_root=data_root,\n",
    "                            return_item_subpath=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.iterator_factory import ARIDCollateFunction\n",
    "# import data.video_transforms as transforms\n",
    "\n",
    "\n",
    "# video_transform=transforms.Compose([\n",
    "#                                     transforms.RandomScale(make_square=True, aspect_ratio=[0.8, 1./0.8], slen=[224, 288]),\n",
    "#                                     transforms.RandomCrop((224, 224)), # insert a resize if needed\n",
    "#                                     transforms.RandomHorizontalFlip(),\n",
    "#                                     transforms.RandomHLS(vars=[15, 35, 25]),\n",
    "#                                     transforms.ToTensor(),\n",
    "#                                     normalize,\n",
    "#                                 ]\n",
    "# aridcollate = ARIDCollateFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 3, 224, 224])\n",
      "tensor([7, 5, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "out = next(iter(train_iter))\n",
    "print(out[0].shape)\n",
    "print(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Turn/Turn_10_19.mp4', 'Run/Run_10_12.mp4', 'Walk/Walk_10_3.mp4', 'Turn/Turn_16_22.mp4')\n"
     ]
    }
   ],
   "source": [
    "print(out[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  9, 10,  5])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:VideoIter:: >> `check_video' is off, `tolerant_corrupted_video' is automatically activated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "dataset = \"ARID\"\n",
    "batch_size = 4\n",
    "clip_length = 16\n",
    "train_frame_interval = 2\n",
    "data_root = \"./dataset/ARID\"\n",
    "config = {}\n",
    "config['mean'] = [0.43216, 0.394666, 0.37645] \n",
    "config['std'] = [0.22803, 0.22145, 0.216989] \n",
    "input_conf = config\n",
    "resume_epoch=0\n",
    "return_item_subpath=True\n",
    "iter_seed = 0 #torch.initial_seed() + 100 + max(0, resume_epoch) * 100\n",
    "\n",
    "\n",
    "from data.iterator_factory import get_arid\n",
    "train_dataset = get_arid(name=dataset, \n",
    "                         clip_length=clip_length, \n",
    "                         train_interval=train_frame_interval,\n",
    "                         mean=input_conf['mean'], std=input_conf['std'], seed=iter_seed, data_root=data_root,\n",
    "                         return_item_subpath=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.iterator_factory import ARIDCollateFunction\n",
    "\n",
    "num_workers = 1\n",
    "batch_size = 2\n",
    "input_size = 224\n",
    "\n",
    "# collate_fn = lightly.data.SimCLRCollateFunction(\n",
    "#     input_size=input_size,\n",
    "#     vf_prob=0.5,\n",
    "#     rr_prob=0.5\n",
    "# )\n",
    "\n",
    "collate_fn = ARIDCollateFunction()\n",
    "\n",
    "dataloader_train_arid = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0][0].shape=torch.Size([16, 3, 224, 224])\n",
      "batch_size=2\n",
      "batch[0][0].dtype=torch.float32\n",
      "0 torch.Size([16, 3, 224, 224])\n",
      "0 torch.float32\n",
      "1 torch.Size([16, 3, 224, 224])\n",
      "1 torch.float32\n",
      "2 torch.Size([16, 3, 224, 224])\n",
      "2 torch.float32\n",
      "3 torch.Size([16, 3, 224, 224])\n",
      "3 torch.float32\n",
      "len(transforms)=4\n",
      "transforms[i].shape=torch.Size([1, 1, 16, 3, 224, 224]), batch[i % batch_size][0].shape=torch.Size([1, 1, 16, 3, 224, 224])\n",
      "transforms[i].shape=torch.Size([1, 1, 16, 3, 224, 224]), batch[i % batch_size][0].shape=torch.Size([1, 1, 16, 3, 224, 224])\n",
      "transforms[i].shape=torch.Size([1, 1, 16, 3, 224, 224]), batch[i % batch_size][0].shape=torch.Size([1, 1, 16, 3, 224, 224])\n",
      "transforms[i].shape=torch.Size([1, 1, 16, 3, 224, 224]), batch[i % batch_size][0].shape=torch.Size([1, 1, 16, 3, 224, 224])\n",
      "batch[0][0].shape=torch.Size([16, 3, 224, 224])\n",
      "batch_size=2\n",
      "batch[0][0].dtype=torch.float32\n",
      "0 torch.Size([16, 3, 224, 224])\n",
      "0 torch.float32\n",
      "1 torch.Size([16, 3, 224, 224])\n",
      "1 torch.float32\n",
      "2 torch.Size([16, 3, 224, 224])\n",
      "2 torch.float32\n",
      "3 torch.Size([16, 3, 224, 224])\n",
      "3 torch.float32\n"
     ]
    }
   ],
   "source": [
    "out = next(iter(dataloader_train_arid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 16, 3, 224, 224])\n",
      "torch.Size([2, 1, 16, 3, 224, 224])\n",
      "tensor([7, 8])\n",
      "['Stand/Stand_6_9.mp4', 'Turn/Turn_11_36.mp4']\n"
     ]
    }
   ],
   "source": [
    "print(out[0][0].shape)\n",
    "print(out[0][1].shape)\n",
    "print(out[1])\n",
    "print(out[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ebf055c2d6178097dd2440a27d31dd6c8b0ce6ba1e5cd9b7b5874fe71052e926"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('lightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
